@article{hinton2015distilling,
  title={Distilling the Knowledge in a Neural Network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@article{ferruz2022protgpt2,
  title={{ProtGPT2} is a deep unsupervised language model for protein design},
  author={Ferruz, Noelia and Schmidt, Steffen and H{\"o}cker, Birte},
  journal={Nature Communications},
  volume={13},
  pages={4348},
  year={2022},
  publisher={Nature Publishing Group}
}

@article{rives2021biological,
  title={Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences},
  author={Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C Lawrence and Ma, Jerry and Fergus, Rob},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={15},
  pages={e2016239118},
  year={2021},
  publisher={National Academy of Sciences}
}

@article{sanh2019distilbert,
  title={{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{brandes2022proteinbert,
  title={{ProteinBERT}: a universal deep-learning model of protein sequence and function},
  author={Brandes, Nadav and Ofer, Dan and Peleg, Yam and Rappoport, Nadav and Linial, Michal},
  journal={Bioinformatics},
  volume={38},
  number={8},
  pages={2102--2110},
  year={2022},
  publisher={Oxford University Press}
}

@article{wang2024mtdp,
  title={{MTDP}: Multi-Teacher Knowledge Distillation for Protein Representations},
  author={Wang, Yijia and others},
  journal={Bioinformatics},
  year={2024}
}

@article{spidergpt2025,
  title={{SpiderGPT}: Knowledge distillation of a spider silk protein language model},
  author={{SpiderGPT Consortium}},
  journal={bioRxiv},
  year={2025}
}

@inproceedings{guo2017calibration,
  title={On Calibration of Modern Neural Networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={Proceedings of the 34th International Conference on Machine Learning (ICML)},
  pages={1321--1330},
  year={2017}
}

@inproceedings{muller2019label,
  title={When Does Label Smoothing Help?},
  author={M{\"u}ller, Rafael and Kornblith, Simon and Hinton, Geoffrey},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={32},
  year={2019}
}

@inproceedings{romero2015fitnets,
  title={{FitNets}: Hints for Thin Deep Nets},
  author={Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
  booktitle={Proceedings of the 3rd International Conference on Learning Representations (ICLR)},
  year={2015}
}

@inproceedings{park2019relational,
  title={Relational Knowledge Distillation},
  author={Park, Wonpyo and Kim, Dongju and Lu, Yan and Cho, Minsu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={3967--3976},
  year={2019}
}

@inproceedings{jiao2020tinybert,
  title={{TinyBERT}: Distilling {BERT} for Natural Language Understanding},
  author={Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={4163--4174},
  year={2020}
}

@article{uniprot2023,
  title={{UniProt}: the Universal Protein Knowledgebase in 2023},
  author={{The UniProt Consortium}},
  journal={Nucleic Acids Research},
  volume={51},
  number={D1},
  pages={D483--D489},
  year={2023},
  publisher={Oxford University Press}
}

@article{lin2023esmfold,
  title={Evolutionary-scale prediction of atomic-level protein structure with a language model},
  author={Lin, Zeming and Akin, Halil and Rao, Roshan and Hie, Brian and Zhu, Zhongkai and Lu, Wenting and Smeber, Nikita and Verkuil, Robert and Kabeli, Ori and Shmueli, Yaniv and dos Santos Costa, Allan and Fazel-Zarandi, Maryam and Sercu, Tom and Candber, Sal and Rives, Alexander},
  journal={Science},
  volume={379},
  number={6637},
  pages={1123--1130},
  year={2023},
  publisher={American Association for the Advancement of Science}
}

@article{madani2023progen,
  title={Large language models generate functional protein sequences across diverse families},
  author={Madani, Ali and Krause, Ben and Greene, Eric R and Subramanian, Subu and Mohr, Benjamin P and Holton, James M and Olmos, Jose Luis and Xiong, Caiming and Sun, Zhz Z and Socher, Richard and Fraser, James S and Naik, Nikhil},
  journal={Nature Biotechnology},
  volume={41},
  pages={1099--1106},
  year={2023},
  publisher={Nature Publishing Group}
}

@inproceedings{vaswani2017attention,
  title={Attention Is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={30},
  year={2017}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI Blog},
  year={2019}
}

@inproceedings{naeini2015ece,
  title={Obtaining Well Calibrated Predictions Using {Bayesian} Binning into Quantiles},
  author={Naeini, Mahdi Pakdaman and Cooper, Gregory F and Hauskrecht, Milos},
  booktitle={Proceedings of the 29th AAAI Conference on Artificial Intelligence},
  pages={2901--2907},
  year={2015}
}
