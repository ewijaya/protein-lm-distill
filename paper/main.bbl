\begin{thebibliography}{14}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ferruz et~al.(2022)Ferruz, Schmidt, and
  H{\"o}cker]{ferruz2022protgpt2}
Noelia Ferruz, Steffen Schmidt, and Birte H{\"o}cker.
\newblock {ProtGPT2} is a deep unsupervised language model for protein design.
\newblock \emph{Nature Communications}, 13:\penalty0 4348, 2022.

\bibitem[Rives et~al.(2021)Rives, Meier, Sercu, Goyal, Lin, Liu, Guo, Ott,
  Zitnick, Ma, and Fergus]{rives2021biological}
Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason
  Liu, Demi Guo, Myle Ott, C~Lawrence Zitnick, Jerry Ma, and Rob Fergus.
\newblock Biological structure and function emerge from scaling unsupervised
  learning to 250 million protein sequences.
\newblock \emph{Proceedings of the National Academy of Sciences}, 118\penalty0
  (15):\penalty0 e2016239118, 2021.

\bibitem[Madani et~al.(2023)Madani, Krause, Greene, Subramanian, Mohr, Holton,
  Olmos, Xiong, Sun, Socher, Fraser, and Naik]{madani2023progen}
Ali Madani, Ben Krause, Eric~R Greene, Subu Subramanian, Benjamin~P Mohr,
  James~M Holton, Jose~Luis Olmos, Caiming Xiong, Zhz~Z Sun, Richard Socher,
  James~S Fraser, and Nikhil Naik.
\newblock Large language models generate functional protein sequences across
  diverse families.
\newblock \emph{Nature Biotechnology}, 41:\penalty0 1099--1106, 2023.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI Blog}, 2019.

\bibitem[{The UniProt Consortium}(2023)]{uniprot2023}
{The UniProt Consortium}.
\newblock {UniProt}: the universal protein knowledgebase in 2023.
\newblock \emph{Nucleic Acids Research}, 51\penalty0 (D1):\penalty0 D483--D489,
  2023.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Brandes et~al.(2022)Brandes, Ofer, Peleg, Rappoport, and
  Linial]{brandes2022proteinbert}
Nadav Brandes, Dan Ofer, Yam Peleg, Nadav Rappoport, and Michal Linial.
\newblock {ProteinBERT}: a universal deep-learning model of protein sequence
  and function.
\newblock \emph{Bioinformatics}, 38\penalty0 (8):\penalty0 2102--2110, 2022.

\bibitem[Wang et~al.(2024)]{wang2024mtdp}
Yijia Wang et~al.
\newblock {MTDP}: Multi-teacher knowledge distillation for protein
  representations.
\newblock \emph{Bioinformatics}, 2024.

\bibitem[{SpiderGPT Consortium}(2025)]{spidergpt2025}
{SpiderGPT Consortium}.
\newblock {SpiderGPT}: Knowledge distillation of a spider silk protein language
  model.
\newblock \emph{bioRxiv}, 2025.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
Chuan Guo, Geoff Pleiss, Yu~Sun, and Kilian~Q Weinberger.
\newblock On calibration of modern neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning (ICML)}, pages 1321--1330, 2017.

\bibitem[M{\"u}ller et~al.(2019)M{\"u}ller, Kornblith, and
  Hinton]{muller2019label}
Rafael M{\"u}ller, Simon Kornblith, and Geoffrey Hinton.
\newblock When does label smoothing help?
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, volume~32, 2019.

\bibitem[Naeini et~al.(2015)Naeini, Cooper, and Hauskrecht]{naeini2015ece}
Mahdi~Pakdaman Naeini, Gregory~F Cooper, and Milos Hauskrecht.
\newblock Obtaining well calibrated predictions using {Bayesian} binning into
  quantiles.
\newblock In \emph{Proceedings of the 29th AAAI Conference on Artificial
  Intelligence}, pages 2901--2907, 2015.

\bibitem[Lin et~al.(2023)Lin, Akin, Rao, Hie, Zhu, Lu, Smeber, Verkuil, Kabeli,
  Shmueli, dos Santos~Costa, Fazel-Zarandi, Sercu, Candber, and
  Rives]{lin2023esmfold}
Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita
  Smeber, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos~Costa,
  Maryam Fazel-Zarandi, Tom Sercu, Sal Candber, and Alexander Rives.
\newblock Evolutionary-scale prediction of atomic-level protein structure with
  a language model.
\newblock \emph{Science}, 379\penalty0 (6637):\penalty0 1123--1130, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, volume~30, 2017.

\end{thebibliography}
