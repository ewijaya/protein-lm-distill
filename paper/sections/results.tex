\section{Results}

\subsection{Ablation reveals complementary effect}

To assess the contribution of each enhancement, we conducted a $2 \times 2$ ablation
study using the Tiny architecture (4 layers, 4 heads, 256 embedding dimensions),
toggling uncertainty-aware position weighting and calibration-aware label smoothing
independently (Table~\ref{tab:ablation}). All four configurations used identical training
hyperparameters ($T = 2.0$, $\alpha = 0.5$, learning rate $= 10^{-3}$, 3 epochs).

The baseline (standard Hinton-style distillation) achieved a perplexity of 18.95.
Applying uncertainty weighting alone degraded perplexity to 36.89 (+95\%), while
calibration smoothing alone degraded it further to 39.64 (+109\%). Both individual
enhancements also increased KL divergence from the teacher and worsened expected
calibration error (ECE). Taken individually, neither enhancement appears beneficial.

However, when both enhancements are applied simultaneously, perplexity drops to 8.93---a
53\% improvement over baseline and a 75--77\% improvement over either individual
enhancement. KL divergence decreases from 2.23 to 1.62, and ECE improves from 0.274 to
0.216. This complementary effect, where two individually harmful modifications combine
to produce substantial improvement, is the central finding of this work (Fig.~\ref{fig:ablation}).

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/pdf/fig1_ablation.pdf}
\caption{Ablation study showing the complementary effect. Each enhancement individually
degrades distillation quality (higher perplexity, higher KL divergence, higher ECE),
but their combination yields a 53\% perplexity improvement over baseline.}
\label{fig:ablation}
\end{figure}

\begin{table}[t]
\centering
\caption{Ablation study results on Tiny architecture (4L/4H/256E). Each enhancement
individually degrades distillation quality, but their combination yields a 53\%
improvement over baseline. PPL: perplexity; KL: KL divergence from teacher;
ECE: expected calibration error.}
\label{tab:ablation}
\begin{tabular}{lcccccc}
\toprule
Configuration & Uncertainty & Calibration & PPL & KL Div & ECE & vs.\ Baseline \\
\midrule
Baseline (standard KD) & \texttimes & \texttimes & 18.95 & 2.23 & 0.274 & --- \\
+Uncertainty only       & \checkmark & \texttimes & 36.89 & 2.87 & 0.325 & +95\% \\
+Calibration only       & \texttimes & \checkmark & 39.64 & 3.00 & 0.319 & +109\% \\
+Both (synergy)         & \checkmark & \checkmark & \textbf{8.93} & \textbf{1.62} & \textbf{0.216} & \textbf{$-$53\%} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Scaling across model sizes}

To test whether the complementary effect generalizes beyond the ablation architecture,
we trained paired baseline and synergy models at three scales: Tiny (20$\times$
compression), Small (9.4$\times$), and Medium (3.8$\times$). Based on the ablation
finding that synergy training requires careful learning rate selection, we adopted a
protocol using approximately half the baseline learning rate with 500 steps of linear
warmup (see Methods for details).

Table~\ref{tab:scaling} and Fig.~\ref{fig:scaling} show that synergy models outperform baselines at all
three scales. The improvement is largest at the highest compression ratio (87\% at
20$\times$ compression for Tiny) and decreases with scale (54\% for Small, 31\% for
Medium). This trend is expected: as student capacity approaches teacher capacity, the
marginal benefit of enhanced distillation diminishes because standard KD already
transfers knowledge effectively.

\begin{table}[t]
\centering
\caption{Scaling results across three model sizes. Synergy models use both
uncertainty weighting and calibration smoothing with adjusted learning rates and warmup.
All synergy models outperform their respective baselines.}
\label{tab:scaling}
\begin{tabular}{llcccc}
\toprule
Scale & Method & Compression & PPL & ECE & Improvement \\
\midrule
\multirow{2}{*}{Tiny (512E)}
  & Baseline & 20$\times$ & 39.91 & 0.345 & --- \\
  & Synergy  & 20$\times$ & \textbf{5.06} & \textbf{0.183} & 87\% \\
\midrule
\multirow{2}{*}{Small (768E)}
  & Baseline & 9.4$\times$ & 15.19 & 0.235 & --- \\
  & Synergy  & 9.4$\times$ & \textbf{7.05} & 0.259 & 54\% \\
\midrule
\multirow{2}{*}{Medium (1024E)}
  & Baseline & 3.8$\times$ & 3.72 & 0.169 & --- \\
  & Synergy  & 3.8$\times$ & \textbf{2.58} & \textbf{0.135} & 31\% \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=0.8\textwidth]{figures/pdf/fig2_scaling.pdf}
\caption{Perplexity ratio (student/teacher) across model scales. Synergy models
outperform baselines at all three compression ratios, with the largest improvement
at the highest compression (20$\times$).}
\label{fig:scaling}
\end{figure}

\subsection{Calibration analysis}

Expected calibration error (ECE) measures the alignment between predicted confidence and
empirical accuracy across binned probability intervals~\cite{guo2017calibration,
naeini2015ece}. We computed ECE with 10-bin quantization on held-out protein sequences
(Fig.~\ref{fig:calibration}).

Synergy models improve calibration at the Tiny scale (ECE 0.183 vs.\ 0.345, a 47\%
reduction) and at the Medium scale (ECE 0.135 vs.\ 0.169, a 20\% reduction). At the
Small scale, however, the synergy model shows a minor ECE regression (0.259 vs.\ 0.235).
This anomaly likely reflects the fact that the Small model was the only scale where no
learning rate reduction was applied; the warmup schedule alone may not fully optimize
calibration. Overall, synergy distillation improves student calibration at 2 of 3
scales, with the largest gains at higher compression ratios where miscalibration risk
is greatest.

\begin{figure}[t]
\centering
\includegraphics[width=0.6\textwidth]{figures/pdf/fig3_calibration.pdf}
\caption{Expected calibration error across model scales. Synergy (red)
achieves lower ECE than Baseline (blue) at Medium and Tiny scales, with the
largest improvement at Tiny (0.183 vs.\ 0.345, 47\% reduction). At Small
scale, Synergy shows a minor regression (0.259 vs.\ 0.235). The teacher ECE
(0.148) is shown for reference.}
\label{fig:calibration}
\end{figure}

\subsection{Biological validity}

For protein language models intended for \textit{de novo} sequence design, preserving
biologically realistic amino acid usage is essential. We evaluated the amino acid
frequency distributions of generated sequences against the natural distribution observed
in UniProt~\cite{uniprot2023} (Fig.~\ref{fig:aa_dist}).

All student models---both baseline and synergy---produce amino acid distributions
closely matching the natural UniProt distribution, with KL divergence below 0.015 in
all cases. Quantifying per-residue deviations via mean absolute deviation (MAD) from
UniProt frequencies reveals a clear ordering: the teacher deviates least
(MAD\,=\,0.0034), followed by the synergy student (MAD\,=\,0.0073), then the baseline
student (MAD\,=\,0.0089). The synergy model is closer to natural on 13 of 20 amino
acids, indicating that the combined enhancements improve distributional fidelity
relative to standard distillation.

\begin{figure}[t]
\centering
\includegraphics[width=0.8\textwidth]{figures/pdf/fig4_aa_distribution.pdf}
\caption{Amino acid frequency deviation from the natural UniProt distribution
(left) with mean absolute deviation summary (right). The synergy student
(MAD\,=\,0.0073) deviates 18\% less than the baseline (MAD\,=\,0.0089),
confirming that the combined enhancements improve distributional fidelity.}
\label{fig:aa_dist}
\end{figure}

\subsection{Compression--quality tradeoff}

Plotting perplexity against compression ratio for all models reveals a Pareto frontier
(Fig.~\ref{fig:pareto}). Synergy models dominate baseline models at every compression level tested,
offering strictly better perplexity at the same model size. The improvement is most
pronounced at high compression ratios, suggesting that the complementary distillation
framework is especially valuable when aggressive compression is required.

\begin{figure}[t]
\centering
\includegraphics[width=0.7\textwidth]{figures/pdf/fig5_pareto.pdf}
\caption{Compression--quality Pareto frontier. Synergy models (filled) dominate
baseline models (open) at every compression ratio, achieving strictly better
perplexity at the same model size.}
\label{fig:pareto}
\end{figure}

\subsection{Practical deployment}

Inference benchmarks on an NVIDIA L40S GPU show that student models achieve substantial
speedups over the teacher (Fig.~\ref{fig:speed}): 5.3$\times$ for Tiny, 4.1$\times$ for Small, and
2.4$\times$ for Medium models. These speedups, combined with dramatically reduced memory
requirements (Fig.~\ref{fig:throughput}), enable protein sequence generation on
consumer-grade GPUs. Peak GPU memory drops from 3.2\,GB for the teacher to just
170\,MB for the Tiny model---a 19$\times$ reduction. At the Tiny model's throughput of
111 sequences per minute, screening a library of $10^6$ candidate sequences---typical
of combinatorial antibody or enzyme engineering campaigns---requires approximately
6 GPU-hours on a single consumer-grade device, compared to $\sim$48 GPU-hours for the
full teacher. The Tiny model's $\sim$37M parameters fit within 170\,MB of GPU memory,
permitting deployment on shared laboratory workstations without dedicated accelerator
infrastructure.

\begin{figure}[t]
\centering
\includegraphics[width=0.7\textwidth]{figures/pdf/fig6_speed.pdf}
\caption{Inference speedup on an NVIDIA L40S GPU. Student models achieve
2.4--5.3$\times$ speedup over the ProtGPT2 teacher, enabling deployment on
consumer-grade hardware.}
\label{fig:speed}
\end{figure}

\subsection{Structural quality}

To assess whether distilled models generate structurally plausible proteins, we scored
50 sequences per model using ESMFold predicted local distance difference test
(pLDDT)~\cite{lin2023esmfold} (Fig.~\ref{fig:plddt}). The teacher achieved a mean pLDDT
of 51.2 with 16\% of sequences exceeding the confident prediction threshold of 70.
Student models scored lower (mean pLDDT 38--40), consistent with the reduced capacity
of smaller architectures. Importantly, synergy and baseline models at the same scale
produce comparable pLDDT distributions (synergy-medium: 38.1 vs.\ baseline-medium: 38.1),
confirming that the complementary distillation framework does not sacrifice structural
quality for improved perplexity. The pLDDT gap between teacher and students reflects
model capacity rather than a training methodology deficit.

\begin{figure}[t]
\centering
\includegraphics[width=0.7\textwidth]{figures/pdf/fig7_plddt.pdf}
\caption{ESMFold pLDDT scores for 50 generated sequences per model. The teacher
achieves higher structural confidence (mean 51.2) due to greater capacity. At the
Medium scale, synergy and baseline produce indistinguishable pLDDT distributions
(both 38.1), confirming that complementary distillation preserves structural
quality---the capacity gap is a compression effect, not a method deficit.}
\label{fig:plddt}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.7\textwidth]{figures/pdf/fig8_throughput.pdf}
\caption{Generation throughput and GPU memory usage on an NVIDIA L40S. The Tiny model
generates 111 sequences per minute while requiring only 170\,MB GPU memory (19$\times$
reduction from the teacher's 3.2\,GB), enabling large-scale screening on consumer
hardware.}
\label{fig:throughput}
\end{figure}
