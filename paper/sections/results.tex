\section{Results}

\subsection{Ablation reveals complementary regularizers}

To assess the contribution of each enhancement, we conducted a $2 \times 2$ ablation
study using a Micro architecture (4 layers, 4 heads, 256 embedding dimensions;
$\sim$16M parameters, $\sim$46$\times$ compression), toggling uncertainty-aware position
weighting and calibration-aware label smoothing independently
(Table~\ref{tab:ablation}). This smaller architecture was chosen to reduce the
computational cost of running all four configurations under identical training
hyperparameters ($T = 2.0$, $\alpha = 0.5$, learning rate $= 10^{-3}$, 3 epochs).

The baseline (standard Hinton-style distillation) achieved a perplexity of 18.95.
Applying uncertainty weighting alone degraded perplexity to 36.89 (+95\%), while
calibration smoothing alone degraded it further to 39.64 (+109\%). Both individual
enhancements also increased KL divergence from the teacher and worsened expected
calibration error (ECE). Taken individually, neither enhancement appears beneficial.

However, when both enhancements are applied simultaneously, perplexity drops to 8.93---a
53\% improvement over baseline and a 75--77\% improvement over either individual
enhancement. KL divergence decreases from 2.23 to 1.62, and ECE improves from 0.274 to
0.216. This complementary-regularizer effect, where two individually harmful modifications combine
to produce substantial improvement, is the central finding of this work (Fig.~\ref{fig:ablation}).

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/pdf/fig1_ablation.pdf}
\caption{Ablation study showing the complementary-regularizer effect. Each enhancement individually
degrades distillation quality (higher perplexity, higher KL divergence, higher ECE),
but their combination yields a 53\% perplexity improvement over baseline.}
\label{fig:ablation}
\end{figure}

\begin{table}[t]
\centering
\caption{Ablation study results on Micro architecture (4L/4H/256E, $\sim$16M
parameters). Each enhancement individually degrades distillation quality, but their
combination yields a 53\% improvement over baseline. PPL: perplexity; KL: KL
divergence from teacher; ECE: expected calibration error.}
\label{tab:ablation}
\begin{tabular}{lcccccc}
\toprule
Configuration & Uncertainty & Calibration & PPL & KL Div & ECE & vs.\ Baseline \\
\midrule
Baseline (standard KD) & \texttimes & \texttimes & 18.95 & 2.23 & 0.274 & --- \\
+Uncertainty only       & \checkmark & \texttimes & 36.89 & 2.87 & 0.325 & +95\% \\
+Calibration only       & \texttimes & \checkmark & 39.64 & 3.00 & 0.319 & +109\% \\
+Both (synergy)         & \checkmark & \checkmark & \textbf{8.93} & \textbf{1.62} & \textbf{0.216} & \textbf{$-$53\%} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Scaling across model sizes}

To test whether the complementary-regularizer effect generalizes beyond the Micro ablation
architecture, we trained paired baseline and synergy models at three larger scales:
Tiny (4L/4H/512E, 20$\times$ compression), Small (9.4$\times$), and Medium
(3.8$\times$). Note that the Tiny model here uses a 512-dimensional embedding,
distinct from the 256-dimensional Micro model used for the ablation. Based on the
ablation finding that synergy training requires careful learning rate selection, we
adopted a protocol using approximately half the baseline learning rate with 500 steps
of linear warmup (see Methods for details).

Table~\ref{tab:scaling} and Fig.~\ref{fig:scaling} show that synergy models outperform baselines at all
three scales. The improvement is largest at the highest compression ratio (87\% at
20$\times$ compression for Tiny) and decreases with scale (54\% for Small, 31\% for
Medium). This trend is expected: as student capacity approaches teacher capacity, the
marginal benefit of enhanced distillation diminishes because standard KD already
transfers knowledge effectively.

\begin{table}[t]
\centering
\caption{Scaling results across three model sizes. Synergy models use both
uncertainty weighting and calibration smoothing with adjusted learning rates and warmup.
All synergy models outperform their respective baselines.}
\label{tab:scaling}
\begin{tabular}{llcccc}
\toprule
Scale & Method & Compression & PPL & ECE & Improvement \\
\midrule
\multirow{2}{*}{Tiny (512E)}
  & Baseline & 20$\times$ & 39.91 & 0.345 & --- \\
  & Synergy  & 20$\times$ & \textbf{5.06} & \textbf{0.183} & 87\% \\
\midrule
\multirow{2}{*}{Small (768E)}
  & Baseline & 9.4$\times$ & 15.19 & 0.235 & --- \\
  & Synergy  & 9.4$\times$ & \textbf{7.05} & 0.259 & 54\% \\
\midrule
\multirow{2}{*}{Medium (1024E)}
  & Baseline & 3.8$\times$ & 3.72 & 0.169 & --- \\
  & Synergy  & 3.8$\times$ & \textbf{2.58} & \textbf{0.135} & 31\% \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=0.8\textwidth]{figures/pdf/fig2_scaling.pdf}
\caption{Perplexity ratio (student/teacher) across model scales. Synergy models
outperform baselines at all three compression ratios, with the largest improvement
at the highest compression (20$\times$).}
\label{fig:scaling}
\end{figure}

\subsection{Calibration analysis}

Expected calibration error (ECE) measures the alignment between predicted confidence and
empirical accuracy across binned probability intervals~\cite{guo2017calibration,
naeini2015ece}. We computed ECE with 10-bin quantization on held-out protein sequences
(Fig.~\ref{fig:calibration}).

Synergy models improve calibration at the Tiny scale (ECE 0.183 vs.\ 0.345, a 47\%
reduction) and at the Medium scale (ECE 0.135 vs.\ 0.169, a 20\% reduction). At the
Small scale, however, the synergy model shows a minor ECE regression (0.259 vs.\ 0.235).
This anomaly likely reflects the fact that the Small model was the only scale where no
learning rate reduction was applied; the warmup schedule alone may not fully optimize
calibration. Overall, synergy distillation improves student calibration at 2 of 3
scales, with the largest gains at higher compression ratios where miscalibration risk
is greatest.

\begin{figure}[t]
\centering
\includegraphics[width=0.6\textwidth]{figures/pdf/fig3_calibration.pdf}
\caption{Expected calibration error across model scales. Synergy (red)
achieves lower ECE than Baseline (blue) at Medium and Tiny scales, with the
largest improvement at Tiny (0.183 vs.\ 0.345, 47\% reduction). At Small
scale, Synergy shows a minor regression (0.259 vs.\ 0.235). The teacher ECE
(0.148) is shown for reference.}
\label{fig:calibration}
\end{figure}

\subsection{Biological validity}

For protein language models intended for \textit{de novo} sequence design, preserving
biologically realistic amino acid usage is essential. We evaluated the amino acid
frequency distributions of generated sequences against the natural distribution observed
in UniProt~\cite{uniprot2023} (Fig.~\ref{fig:aa_dist}).

All student models---both baseline and synergy---produce amino acid distributions
closely matching the natural UniProt distribution, with KL divergence below 0.015 in
all cases. Quantifying per-residue deviations via mean absolute deviation (MAD) from
UniProt frequencies reveals a clear ordering: the teacher deviates least
(MAD\,=\,0.0034), followed by the synergy student (MAD\,=\,0.0073), then the baseline
student (MAD\,=\,0.0089). The synergy model is closer to natural on 13 of 20 amino
acids, indicating that the combined enhancements improve distributional fidelity
relative to standard distillation.

\begin{figure}[t]
\centering
\includegraphics[width=0.8\textwidth]{figures/pdf/fig4_aa_distribution.pdf}
\caption{Amino acid frequency deviation from the natural UniProt distribution
(left) with mean absolute deviation summary (right). The synergy student
(MAD\,=\,0.0073) deviates 18\% less than the baseline (MAD\,=\,0.0089),
confirming that the combined enhancements improve distributional fidelity.}
\label{fig:aa_dist}
\end{figure}

\subsection{Compression--quality tradeoff}

Plotting perplexity against compression ratio for all models reveals a Pareto frontier
(Fig.~\ref{fig:pareto}). Synergy models dominate baseline models at every compression level tested,
offering strictly better perplexity at the same model size. The improvement is most
pronounced at high compression ratios, suggesting that complementary-regularizer
distillation is especially valuable when aggressive compression is required.

\begin{figure}[t]
\centering
\includegraphics[width=0.7\textwidth]{figures/pdf/fig5_pareto.pdf}
\caption{Compression--quality Pareto frontier. Synergy models (filled) dominate
baseline models (open) at every compression ratio, achieving strictly better
perplexity at the same model size.}
\label{fig:pareto}
\end{figure}

\subsection{Practical deployment}

Inference benchmarks on an NVIDIA L40S GPU show that student models achieve substantial
speedups over the teacher (Fig.~\ref{fig:speed}): 5.3$\times$ for Tiny, 4.1$\times$ for Small, and
2.4$\times$ for Medium models. These speedups, combined with dramatically reduced memory
requirements (Fig.~\ref{fig:throughput}), enable protein sequence generation on
consumer-grade GPUs. Peak GPU memory drops from 3.2\,GB for the teacher to just
170\,MB for the Tiny model---a 19$\times$ reduction. At the Tiny model's throughput of
111 sequences per minute, screening a library of $10^6$ candidate sequences---typical
of combinatorial antibody or enzyme engineering campaigns---requires approximately
6 GPU-hours on a single consumer-grade device, compared to $\sim$48 GPU-hours for the
full teacher. The Tiny model's $\sim$37M parameters fit within 170\,MB of GPU memory,
permitting deployment on shared laboratory workstations without dedicated accelerator
infrastructure.

\begin{figure}[t]
\centering
\includegraphics[width=0.7\textwidth]{figures/pdf/fig6_speed.pdf}
\caption{Inference speedup on an NVIDIA L40S GPU. Student models achieve
2.4--5.3$\times$ speedup over the ProtGPT2 teacher, enabling deployment on
consumer-grade hardware.}
\label{fig:speed}
\end{figure}

\subsection{Structural quality}

To assess whether distilled models generate structurally plausible proteins, we scored
50 sequences per model using ESMFold predicted local distance difference test
(pLDDT)~\cite{lin2023esmfold} (Fig.~\ref{fig:plddt}). The teacher achieved a mean pLDDT
of 51.2 with 16\% of sequences exceeding the confident prediction threshold of 70.
Student models scored lower (mean pLDDT 38--40), consistent with the reduced capacity
of smaller architectures. Importantly, synergy and baseline models at the same scale
produce comparable pLDDT distributions (synergy-medium: 38.1 vs.\ baseline-medium: 38.1),
confirming that complementary-regularizer distillation does not sacrifice structural
quality for improved perplexity. The pLDDT gap between teacher and students reflects
model capacity rather than a training methodology deficit.

\begin{figure}[t]
\centering
\includegraphics[width=0.7\textwidth]{figures/pdf/fig7_plddt.pdf}
\caption{ESMFold pLDDT scores for 50 generated sequences per model. The teacher
achieves higher structural confidence (mean 51.2) due to greater capacity. At the
Medium scale, synergy and baseline produce indistinguishable pLDDT distributions
(both 38.1), confirming that complementary-regularizer distillation preserves structural
quality---the capacity gap is a compression effect, not a method deficit.}
\label{fig:plddt}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.7\textwidth]{figures/pdf/fig8_throughput.pdf}
\caption{Generation throughput and GPU memory usage on an NVIDIA L40S. The Tiny model
generates 111 sequences per minute while requiring only 170\,MB GPU memory (19$\times$
reduction from the teacher's 3.2\,GB), enabling large-scale screening on consumer
hardware.}
\label{fig:throughput}
\end{figure}

\subsection{Domain-specific fine-tuning}
\label{sec:finetuning_results}

A critical question for biopharma applications is whether distilled models can serve as
effective starting points for fine-tuning on proprietary datasets. We fine-tuned all five
models---Teacher, Medium, Small, Tiny (synergy), and Baseline-Tiny (standard
KD)---on conotoxin (PF02950, median 68~AA) and lysozyme (PF00959, median 170~AA) at
five training set sizes ($N \in \{50, 100, 200, 500, 1000\}$), totaling 75 runs.
We evaluated test perplexity and HMMER hit rate (sequences matching the family Pfam
profile at $E < 10^{-5}$). AMP results, where the teacher dominates and no HMMER
profile is available, are reported in the Appendix (Table~\ref{tab:ft_amp}).

On conotoxin, all distilled students achieve lower test perplexity than the teacher at
every training set size (Fig.~\ref{fig:finetune_efficiency}a). The gap is largest at
small $N$: at $N = 50$, the teacher reaches a perplexity of 1{,}659 while the Medium
student achieves 372 (4.5$\times$ lower). Medium at $N = 50$ outperforms the teacher at
$N = 100$ (perplexity 1{,}153), demonstrating 2$\times$ sample efficiency. The advantage
persists at large $N$: at $N = 1{,}000$, Medium achieves a perplexity of 30 versus the
teacher's 54.

On lysozyme, the teacher achieves lower test perplexity at every $N$
(Fig.~\ref{fig:finetune_efficiency}b). Yet students generate sequences that more
frequently match the PF00959 HMM profile (Fig.~\ref{fig:finetune_hitrate}a). At
$N = 1{,}000$, Small achieves a 94\% HMMER hit rate versus the teacher's 69\%
(+25 percentage points). At $N = 200$, the gap is even wider: Small 73\% versus
teacher 28\%. This decoupling between perplexity and family-specific generation
quality suggests that distilled representations capture family-level structural
patterns more effectively during fine-tuning.

On conotoxin HMMER hit rate, Medium dominates: 42.5\% at $N = 1{,}000$ versus the
teacher's 8.0\% (Fig.~\ref{fig:finetune_hitrate}b). Hit rates are lower overall for
conotoxin, reflecting the short peptide length (median 68~AA) relative to the
generation length (200 tokens); Medium nonetheless leads on both perplexity and HMMER
for this family.

The fine-tuning advantage is not limited to one model size: all three synergy students
(Medium, Small, Tiny) outperform the teacher on conotoxin perplexity and lysozyme
HMMER hit rate at every $N$ tested. At $N = 1{,}000$, conotoxin perplexity is 30
(Medium), 39 (Small), and 40 (Tiny), all below the teacher's 54; Baseline-Tiny (52)
barely edges out the teacher (Table~\ref{tab:synergy_vs_baseline}). On lysozyme HMMER
at $N = 1{,}000$, Small reaches 94\%, Tiny 84\%, and Medium 83.5\%, all far exceeding
the teacher's 69\%; Baseline-Tiny (71\%) performs at teacher level. The controlled
comparison---Synergy-Tiny versus Baseline-Tiny (same 37M architecture, different
distillation method)---isolates the source of the advantage: Baseline-Tiny performs
near teacher level while Synergy-Tiny far exceeds both, with 15 out of 15 perplexity
wins across all families. This confirms that the synergy distillation procedure, not
just model compression, drives the fine-tuning advantage.

Students fine-tune 20--162$\times$ faster than the teacher in wall-clock time. The Tiny
model completes AMP fine-tuning ($N = 1{,}000$) in 25 seconds versus 66 minutes for the
teacher. No gradient checkpointing is required for any student, making fine-tuning
feasible on consumer GPUs.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/pdf/fig10_finetune_efficiency.pdf}
\caption{Test perplexity versus training set size for domain-specific fine-tuning.
(a)~Conotoxin: all distilled students achieve lower perplexity than the teacher at
every $N$, with Medium at $N = 50$ outperforming the teacher at $N = 100$
(2$\times$ sample efficiency). (b)~Lysozyme: the teacher achieves lower perplexity,
but students generate more family-matching sequences (see Fig.~\ref{fig:finetune_hitrate}).}
\label{fig:finetune_efficiency}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/pdf/fig11_finetune_hitrate.pdf}
\caption{HMMER hit rate versus training set size. (a)~Lysozyme: Small achieves 94\%
hit rate at $N = 1{,}000$ versus the teacher's 69\%, despite having higher perplexity.
(b)~Conotoxin: Medium reaches 42.5\% versus the teacher's 8.0\%; lower absolute
rates reflect the generation length mismatch relative to the short peptide family.}
\label{fig:finetune_hitrate}
\end{figure}

\begin{table}[t]
\centering
\caption{Synergy-Tiny versus Baseline-Tiny at $N = 1{,}000$: same 37M architecture,
different distillation method. Synergy-Tiny outperforms the teacher on conotoxin PPL
and lysozyme HMMER, while Baseline-Tiny performs near teacher level, confirming
that the synergy distillation procedure drives the fine-tuning advantage.}
\label{tab:synergy_vs_baseline}
\begin{tabular}{lcccc}
\toprule
 & \multicolumn{2}{c}{Conotoxin} & \multicolumn{2}{c}{Lysozyme} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
Model & PPL & HMMER (\%) & PPL & HMMER (\%) \\
\midrule
Teacher (738M)       & 54  & 8.0  & \textbf{187} & 69.0 \\
Synergy-Tiny (37M)   & \textbf{40}  & 8.0  & 327 & \textbf{84.0} \\
Baseline-Tiny (37M)  & 52  & 13.0 & 569 & 71.0 \\
\bottomrule
\end{tabular}
\end{table}
