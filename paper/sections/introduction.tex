\section{Introduction}

Protein language models (pLMs) trained on evolutionary sequence data now enable
computational protein design~\cite{ferruz2022protgpt2,
rives2021biological, madani2023progen}. By learning the statistical patterns of natural
protein sequences, autoregressive pLMs can generate novel sequences \textit{de novo}
with properties resembling those found in nature~\cite{ferruz2022protgpt2}. Among these,
ProtGPT2---a GPT-2 architecture model~\cite{radford2019language} with 738 million
parameters trained on UniRef50~\cite{uniprot2023}---has demonstrated the ability to
produce sequences with natural amino acid distributions, plausible secondary structure
content, and globular characteristics.

However, the computational cost of large pLMs creates a significant barrier to practical
deployment. ProtGPT2 requires high-end GPUs for inference, generates sequences at
limited throughput ($\sim$3 seconds per sequence), and cannot be deployed on edge devices
or in resource-constrained laboratory settings. These constraints are particularly
acute in biopharma applications such as ML-guided antibody affinity
maturation~\cite{hie2024antibodies} and protein engineering
campaigns~\cite{madani2023progen}, where evaluating thousands to millions of
candidate sequences demands both high throughput and cost-effective deployment.

Beyond inference efficiency, a critical question for biopharma applications is whether
compressed models can serve as effective starting points for domain-specific fine-tuning
on proprietary datasets, typically comprising only 50--1{,}000 sequences from a target
protein family. If distillation preserves or enhances the representations needed for
rapid domain adaptation, the same compressed model can serve both as a fast inference
engine and as a sample-efficient fine-tuning base.

Knowledge distillation~\cite{hinton2015distilling} offers a principled approach to model
compression by training a smaller student model to mimic the probability distributions
of a larger teacher model. The key insight of Hinton et al.\ is that temperature-softened
output distributions encode rich inter-class relationships---``dark knowledge''---that
one-hot labels cannot convey. For protein sequences, these soft distributions capture
amino acid substitution patterns: the teacher's prediction that position $t$ should be
leucine, with isoleucine and valine as secondary preferences, conveys evolutionary
constraints that a hard label alone cannot express.

In natural language processing, distillation has produced compact yet performant models
such as DistilBERT~\cite{sanh2019distilbert} and TinyBERT~\cite{jiao2020tinybert},
motivating similar approaches for protein language models.
DistilProtBert~\cite{geffen2022distilprotbert} compressed ProtBert using response-based
distillation, and MTDP~\cite{wang2024mtdp} introduced multi-teacher distillation for
protein representations. For causal protein LMs, Dubey et
al.~\cite{dubey2025spidersilk} applied distillation to a domain-specific model trained
on 572 spider silk sequences. However, \emph{no systematic study has addressed
distillation for general-purpose autoregressive protein language models}, despite these
being the models required for open-ended \textit{de novo} sequence design.

We address this gap with a distillation framework that combines standard Hinton-style
knowledge distillation with two protein-specific enhancements: (1) \textit{uncertainty-aware
position weighting}, which uses teacher entropy to emphasize biologically variable
regions during distillation, and (2) \textit{calibration-aware label smoothing}, which
applies confidence-dependent smoothing to teacher distributions to improve student
calibration~\cite{guo2017calibration, muller2019label}. Our central finding is a
\emph{complementary regularizers}: uncertainty weighting alone increases perplexity by 95\%
and calibration smoothing alone increases it by 109\%, yet their combination improves
perplexity by 53\% over baseline distillation. We provide a mechanistic explanation
grounded in information theory: smoothing acts as a noise filter on teacher distributions,
while weighting amplifies the cleaned signal at biologically important positions.

Our contributions are as follows:
\begin{enumerate}
  \item The first systematic study of knowledge distillation for general-purpose
    autoregressive protein language models.
  \item Two protein-specific distillation enhancements: uncertainty-aware position
    weighting and calibration-aware label smoothing.
  \item Discovery and mechanistic explanation of complementary regularizers, where
    individually harmful modifications combine for substantial improvement.
  \item A comprehensive evaluation framework spanning perplexity, calibration (ECE),
    amino acid distributional fidelity, and inference benchmarks.
  \item Open-source compressed models at three scales (37M, 78M, 194M parameters)
    available on HuggingFace.
  \item Demonstration that distilled students are superior fine-tuning starting points
    on scarce protein family data, achieving higher sample efficiency and better
    family-specific generation than the teacher.
\end{enumerate}
