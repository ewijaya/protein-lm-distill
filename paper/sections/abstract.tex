\begin{abstract}
Large autoregressive protein language models (700M+ parameters) generate novel
sequences \textit{de novo}, yet their computational demands limit throughput in
biopharma workflows such as antibody affinity maturation and enzyme engineering,
where millions of candidates must be evaluated. We present a distillation
framework combining temperature-scaled knowledge distillation with two
protein-specific enhancements: uncertainty-aware position weighting, which
emphasizes biologically variable regions via teacher entropy, and
calibration-aware label smoothing. A central finding is \emph{complementary
regularization}: each enhancement individually degrades distillation quality, yet their
combination yields 53\% perplexity improvement over baseline distillation---a
result we explain through information-theoretic analysis of noise amplification
and signal filtering. We train three student models at 3.8--20$\times$
compression, all outperforming baselines (31--87\% perplexity improvement). The
resulting models achieve 2.4--5.3$\times$ speedup while preserving amino acid
distributions consistent with natural proteins (KL divergence $< 0.015$),
enabling on-premise screening of proprietary sequences without cloud API
dependencies---a key requirement in pharmaceutical settings.
\end{abstract}
