\begin{abstract}
Large autoregressive protein language models generate novel sequences
\textit{de novo}, but their size limits throughput and precludes rapid domain
adaptation on scarce proprietary data. We distill ProtGPT2 into compact
students using two protein-specific enhancements, uncertainty-aware position
weighting and calibration-aware label smoothing, that individually degrade
quality yet combine for substantial improvement. We trace this
\emph{complementary-regularizer} effect to information theory: smoothing
denoises teacher distributions while weighting amplifies the cleaned signal
at biologically variable positions. Students are several times faster at
inference, preserve natural amino acid distributions, and use a fraction of
the teacher's memory, enabling deployment on consumer-grade hardware. When
fine-tuned on protein families with as few as 50 sequences, students generate
more family-matching sequences than the teacher, achieving higher sample
efficiency and Pfam hit rates despite their smaller capacity. These results
establish distilled protein language models as superior starting points for
domain adaptation on scarce data.
\end{abstract}
